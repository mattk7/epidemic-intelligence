{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82146fbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Curve Boxplots\n",
    "\n",
    "TODO: \n",
    "\n",
    "- <s>Roll `query_base` so my eyes work again.</s>\n",
    "\n",
    "- <s>Consolidate step 1.</s>\n",
    "\n",
    "- <s>Seperate grouping and centrality metrics.</s> \n",
    "\n",
    "- <s>Band depth/MBD implementation: O(n<sup>3</sup>)? Centrality only, not distance.</s> \n",
    "\n",
    "- <s>Allow for efficiency without grouping.</s>\n",
    "\n",
    "- Optimization: do the math on runtime reductions from clustering/partitioning. Partitioning by date might be useless? `PERCENTILE_CONT` &#8594; `APPROX_QUANTILE`?\n",
    "\n",
    "- Write graphing package to layer graphs.\n",
    "\n",
    "- Grouping by derivative? Is this stupid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c5c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64e605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff452fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb619db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bin_builder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmcolors\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbin_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_country_query\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bin_builder'"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.colors as mcolors\n",
    "from bin_builder import build_country_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02503a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for grouping\n",
    "num_clusters = 2  # Example number of clusters\n",
    "num_features = 5   # Set the number of features to select\n",
    "grouping_method = 'mse' # 'mse' or 'abc'\n",
    "centrality_method = 'mbd' # 'mse' for mean squared error,'abc' for area between the curves or 'mbd' for modified band depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af772a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting\n",
    "def color_to_rgba(color_name, value, alpha):\n",
    "    # Get the RGB components of the color\n",
    "    rgb = mcolors.to_rgb(color_name)\n",
    "    \n",
    "    # Calculate the gray value based on the input value\n",
    "    gray = (1 - value) * 0.8  # This adjusts how gray the color will be\n",
    "\n",
    "    # Compute the final RGBA values\n",
    "    r = rgb[0] * value + gray\n",
    "    g = rgb[1] * value + gray\n",
    "    b = rgb[2] * value + gray\n",
    "    \n",
    "    # Return the RGBA string\n",
    "#     print(f'rgba({int(r * 255)}, {int(g * 255)}, {int(b * 255)}, {alpha})')\n",
    "    return f'rgba({int(r * 255)}, {int(g * 255)}, {int(b * 255)}, {alpha})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f143e9e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# the location of our credentials json and name of bigquery project\n",
    "credentials = service_account.Credentials.from_service_account_file('C:\\\\Users\\\\elija\\\\Documents\\\\24f-coop\\\\credentials.json')\n",
    "project = 'net-data-viz-handbook'\n",
    "\n",
    "# Initialize a GCS client\n",
    "client = bigquery.Client(credentials=credentials, project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d4f64",
   "metadata": {},
   "source": [
    "## Procedurally generating queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa011d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'net-data-viz-handbook.sri_data.SIR_0_countries_incidence_daily'\n",
    "country_ids = [215] \n",
    "run_ids = 'all'\n",
    "min_age = 0\n",
    "max_age = 17\n",
    "categories = ['Infectious']\n",
    "q = build_country_query(table, country_ids, run_ids, min_age, max_age, categories, grouped=True)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d2c86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# this is the query\n",
    "query_base =  \"\"\"\n",
    "CREATE OR REPLACE TABLE sri_data.infectious_data\n",
    "PARTITION BY date\n",
    "CLUSTER BY country_id, run_id AS\n",
    "SELECT \n",
    "    date,\n",
    "    country_id,\n",
    "    run_id,\n",
    "    -- Calculate 7-day rolling average of total_infectious\n",
    "    AVG(SUM(Infectious_13_17 + Infectious_18_23)) OVER (\n",
    "        PARTITION BY country_id, run_id \n",
    "        ORDER BY date \n",
    "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "    ) AS total_infectious\n",
    "FROM `net-data-viz-handbook.sri_data.SIR_0_countries_incidence_daily`\n",
    "WHERE country_id IN (218)\n",
    "  AND run_id BETWEEN 1 AND 100\n",
    "GROUP BY date, country_id, run_id;\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(query_base).result()  # Execute the query to create the table\n",
    "print(\"Data sliced successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bb745",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KMeans and a prayer (for creating grouped curve-based statistics)\n",
    "- Uses the first num_features columns to simplify the KMeans algorithms, which is O(n<sup>2</sup>)?\n",
    "    - Might just not need PCA or similar\n",
    "- Procedural once distance matrix is computed (eg have to query main table manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4d9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ed9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8284a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69709952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the curve distance table\n",
    "query_distances = \"\"\"\n",
    "CREATE OR REPLACE TABLE sri_data.curve_distances AS\n",
    "SELECT\n",
    "    a.run_id AS run_id_a,\n",
    "    b.run_id AS run_id_b,\n",
    "    AVG(POW(a.total_infectious - b.total_infectious, 2)) AS mse,\n",
    "    SUM(ABS(a.total_infectious - b.total_infectious)) AS abc\n",
    "FROM\n",
    "    sri_data.infectious_data a\n",
    "JOIN\n",
    "    sri_data.infectious_data b\n",
    "ON\n",
    "    a.date = b.date\n",
    "GROUP BY\n",
    "    run_id_a, run_id_b\n",
    "\"\"\"\n",
    "client.query(query_distances).result()  # Execute the query to create the table\n",
    "print(\"Curve distance table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create the distance matrix\n",
    "query_distance_matrix = f\"\"\"\n",
    "CREATE OR REPLACE TABLE sri_data.distance_matrix\n",
    "CLUSTER BY run_id AS --optimizations\n",
    "SELECT\n",
    "    run_id_a AS run_id,\n",
    "        ARRAY_AGG(STRUCT(run_id_b, {grouping_method}) ORDER BY run_id_b ASC) AS distances\n",
    "FROM\n",
    "    sri_data.curve_distances\n",
    "GROUP BY\n",
    "    run_id_a;\n",
    "\"\"\"\n",
    "client.query(query_distance_matrix).result()  # Execute the query to create the table\n",
    "print(\"Distance matrix table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce686e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle case when num_clusters = 1\n",
    "if num_clusters == 1:\n",
    "    query_assign_all_to_one_cluster = \"\"\"\n",
    "    CREATE OR REPLACE TABLE sri_data.kmeans_results\n",
    "    CLUSTER BY CENTROID_ID, run_id AS\n",
    "    SELECT DISTINCT\n",
    "        run_id,\n",
    "        1 AS centroid_id  -- Assign all runs to centroid 1\n",
    "    FROM \n",
    "        sri_data.distance_matrix\n",
    "    \"\"\"\n",
    "    s = time.time()\n",
    "    client.query(query_assign_all_to_one_cluster).result()  # Execute the query to assign all runs to centroid 1\n",
    "    print(f\"All runs assigned to centroid 1 successfully in {round(time.time() - s, 3)} seconds.\")\n",
    "    \n",
    "else:\n",
    "    # Step 4: Create the K-means model by selecting the first num_features features based on actual distances\n",
    "    query_create_model = f\"\"\"\n",
    "    CREATE OR REPLACE MODEL sri_data.kmeans_model\n",
    "    OPTIONS(model_type='kmeans', num_clusters={num_clusters}) AS\n",
    "    SELECT\n",
    "        run_id,\n",
    "        ARRAY(\n",
    "            SELECT distance.{grouping_method} \n",
    "            FROM UNNEST(distances) AS distance \n",
    "            WHERE distance.run_id_b <= {num_features}  -- Select only the first num_features\n",
    "        ) AS features\n",
    "    FROM\n",
    "        sri_data.distance_matrix;\n",
    "    \"\"\"\n",
    "    s = time.time()\n",
    "    client.query(query_create_model).result()  # Execute the model creation\n",
    "    print(f\"K-means model created successfully in {round(time.time() - s, 3)} seconds.\")\n",
    "\n",
    "    # Step 5: Apply K-means clustering and save results in a table\n",
    "    query_kmeans = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE sri_data.kmeans_results\n",
    "    CLUSTER BY CENTROID_ID, run_id AS\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        ML.PREDICT(MODEL sri_data.kmeans_model,\n",
    "            (SELECT\n",
    "                run_id,\n",
    "                ARRAY(\n",
    "                    SELECT distance.{grouping_method} \n",
    "                    FROM UNNEST(distances) AS distance \n",
    "                    WHERE distance.run_id_b <= {num_features}  \n",
    "                ) AS features\n",
    "             FROM\n",
    "                sri_data.distance_matrix)\n",
    "        ) AS predictions\n",
    "    \"\"\"\n",
    "    s = time.time()\n",
    "    client.query(query_kmeans).result()  # Execute the model creation\n",
    "    print(f\"K-means clustering results saved successfully in {round(time.time() - s, 3)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6547c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised step 6, gets summed distances for abc and mse\n",
    "s = time.time()\n",
    "save_sum_distances = f\"\"\"CREATE OR REPLACE TABLE `sri_data.total_distances_table`\n",
    "    CLUSTER BY CENTROID_ID, run_id AS\n",
    "    WITH a AS (\n",
    "        SELECT \n",
    "            kr.CENTROID_ID,\n",
    "            kr.run_id, \n",
    "            run_id_b, \n",
    "            {centrality_method}  \n",
    "        FROM \n",
    "            `sri_data.kmeans_results` AS kr\n",
    "        JOIN \n",
    "            `sri_data.curve_distances` AS dm\n",
    "        ON \n",
    "            kr.run_id = dm.run_id_a\n",
    "--        CROSS JOIN \n",
    "--            UNNEST(dm.distances) AS dm_dist  -- Unnest the distances array here \n",
    "    ),\n",
    "    b AS (\n",
    "        SELECT\n",
    "            run_id AS run_id_b, \n",
    "            CENTROID_ID AS CENTROID_ID_B\n",
    "        FROM\n",
    "            `sri_data.kmeans_results`\n",
    "    )\n",
    "    SELECT \n",
    "        a.run_id,\n",
    "        a.CENTROID_ID,\n",
    "        AVG({centrality_method}) AS total_distance  \n",
    "    FROM \n",
    "        a\n",
    "    JOIN \n",
    "        b\n",
    "    ON \n",
    "        a.run_id_b = b.run_id_b\n",
    "    WHERE\n",
    "        a.CENTROID_ID = b.CENTROID_ID_B\n",
    "    GROUP BY\n",
    "        a.CENTROID_ID,\n",
    "        a.run_id;\n",
    "    \"\"\"\n",
    "if centrality_method in ['abc', 'mse']:\n",
    "    client.query(save_sum_distances).result()  # Execute the model creation\n",
    "    print(f\"Distance sum results using {centrality_method.upper()} saved successfully in {round(time.time()-s, 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdfe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "mbd = \"\"\"\n",
    "CREATE OR REPLACE TABLE `sri_data.total_distances_table`\n",
    "CLUSTER BY CENTROID_ID, run_id AS\n",
    "WITH curve_data AS (\n",
    "    SELECT DISTINCT\n",
    "        a.date AS date,\n",
    "        a.run_id AS run_id,\n",
    "        kra.CENTROID_ID as CENTROID_ID,\n",
    "        b.run_id AS boundary_1_id,\n",
    "        c.run_id AS boundary_2_id, \n",
    "        MAX(a.total_infectious) AS curve,\n",
    "        MAX(b.total_infectious) AS boundary_1,\n",
    "        MAX(c.total_infectious) AS boundary_2\n",
    "    FROM\n",
    "        sri_data.infectious_data AS a\n",
    "    JOIN\n",
    "        sri_data.infectious_data AS b ON a.date = b.date\n",
    "    JOIN\n",
    "        sri_data.infectious_data AS c ON a.date = c.date\n",
    "    JOIN \n",
    "        sri_data.kmeans_results AS kra ON a.run_id = kra.run_id\n",
    "    JOIN \n",
    "        sri_data.kmeans_results AS krb ON b.run_id = krb.run_id\n",
    "    JOIN\n",
    "        sri_data.kmeans_results AS krc ON c.run_id = krc.run_id\n",
    "    \n",
    "    WHERE\n",
    "        b.run_id < c.run_id\n",
    "        AND a.run_id != b.run_id\n",
    "        AND a.run_id != c.run_id\n",
    "        AND kra.CENTROID_ID = krb.CENTROID_ID\n",
    "        AND kra.CENTROID_ID = krc.CENTROID_ID\n",
    "\n",
    "    GROUP BY\n",
    "      a.date, a.run_id, CENTROID_ID, b.run_id, c.run_id\n",
    "    ORDER BY\n",
    "      a.run_id, b.run_id, c.run_id, a.date\n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "    run_id,\n",
    "    CENTROID_ID,\n",
    "    1 / COUNT(*) as total_distance\n",
    "FROM curve_data\n",
    "WHERE\n",
    "  (curve_data.boundary_1 <= curve AND curve <= boundary_2)\n",
    "  OR (curve_data.boundary_2 <= curve AND curve <= boundary_1)\n",
    "GROUP BY run_id, CENTROID_ID\n",
    "\"\"\"\n",
    "if centrality_method == 'mbd':\n",
    "    df = client.query(mbd).result().to_dataframe()  # Execute the query to create the table\n",
    "    print(f\"MBD calculated in {round(time.time()-s, 3)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf153ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7\n",
    "query_non_outliers = \"\"\"\n",
    "CREATE OR REPLACE TABLE `sri_data.non_outliers_table` AS\n",
    "WITH iqr_bounds AS (\n",
    "  SELECT \n",
    "    CENTROID_ID,\n",
    "    --Using approximate quantiles to save some time and space\n",
    "    APPROX_QUANTILES(total_distance, 100)[OFFSET(25)] AS lower_quartile,\n",
    "    APPROX_QUANTILES(total_distance, 100)[OFFSET(75)] AS upper_quartile\n",
    "  FROM `sri_data.total_distances_table`\n",
    "  GROUP BY CENTROID_ID\n",
    "),\n",
    "non_outliers AS (\n",
    "  SELECT \n",
    "    d.CENTROID_ID,\n",
    "    d.run_id\n",
    "  FROM `sri_data.total_distances_table` d\n",
    "  JOIN iqr_bounds b\n",
    "    ON d.CENTROID_ID = b.CENTROID_ID\n",
    "  WHERE d.total_distance BETWEEN \n",
    "        (b.lower_quartile - 1.5 * (b.upper_quartile - b.lower_quartile)) \n",
    "        AND (b.upper_quartile + 1.5 * (b.upper_quartile - b.lower_quartile))\n",
    ")\n",
    "\n",
    "SELECT * FROM non_outliers;\n",
    "\n",
    "    \"\"\"\n",
    "client.query(query_non_outliers).result()  # Execute the model creation\n",
    "print(f\"Non-outliers saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca997c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_middle_curves = \"\"\"\n",
    "CREATE OR REPLACE TABLE `sri_data.middle_curves` AS\n",
    "WITH grouped_data AS (\n",
    "  SELECT \n",
    "    CENTROID_ID,\n",
    "    run_id,\n",
    "    total_distance,\n",
    "    ROW_NUMBER() OVER (PARTITION BY CENTROID_ID ORDER BY total_distance) AS rn,\n",
    "    COUNT(*) OVER (PARTITION BY CENTROID_ID) AS total_count\n",
    "  FROM `sri_data.total_distances_table`\n",
    "),\n",
    "top_half AS (\n",
    "  SELECT \n",
    "    CENTROID_ID,\n",
    "    run_id,\n",
    "    total_distance,\n",
    "    rn\n",
    "  FROM grouped_data\n",
    "  WHERE rn <= CAST(total_count * 0.5 AS INT64)  -- Select the top 50% based on total_distance\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    CENTROID_ID,\n",
    "    run_id,\n",
    "FROM top_half;  -- Select the required fields for the middle_curves table\n",
    "\n",
    "\"\"\"\n",
    "client.query(query_middle_curves).result()  # Execute the model creation\n",
    "print(f\"Middle curves saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc98dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_median = \"\"\"\n",
    "CREATE OR REPLACE TABLE `sri_data.median_curves` AS\n",
    "WITH ranked_data AS (\n",
    "  SELECT \n",
    "    CENTROID_ID,\n",
    "    run_id,\n",
    "    total_distance,\n",
    "    ROW_NUMBER() OVER (PARTITION BY CENTROID_ID ORDER BY total_distance) AS rn\n",
    "  FROM `sri_data.total_distances_table`\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    CENTROID_ID,\n",
    "    run_id,\n",
    "FROM ranked_data\n",
    "WHERE rn = 1;  -- Select the run_id with the lowest total_distance for each CENTROID_ID\n",
    "\"\"\"\n",
    "\n",
    "client.query(save_median).result()  # Execute the model creation\n",
    "print(f\"Median curves saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384edfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "get_median_curves = \"\"\"-- Step 3: Calculate min and max values at each time step using the non-outliers table\n",
    "    SELECT\n",
    "        data.date,\n",
    "        mc.CENTROID_ID,\n",
    "        MAX(data.total_infectious) as median\n",
    "    FROM\n",
    "        sri_data.infectious_data as data\n",
    "    JOIN\n",
    "        `sri_data.median_curves` as mc\n",
    "    ON\n",
    "        data.run_id = mc.run_id\n",
    "    GROUP BY\n",
    "        date, \n",
    "        CENTROID_ID\n",
    "    ORDER BY\n",
    "        CENTROID_ID, \n",
    "        date;\n",
    "    \"\"\"\n",
    "plt_median = client.query(get_median_curves).to_dataframe()  # Execute and fetch results\n",
    "print(\"Curves extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "get_curves = \"\"\"-- Step 3: Calculate min and max values at each time step using the non-outliers table\n",
    "    \n",
    "    SELECT\n",
    "        data.date,\n",
    "        nout.CENTROID_ID,\n",
    "        MAX(data.total_infectious) as curve_100,\n",
    "        MIN(data.total_infectious) as curve_0\n",
    "    FROM\n",
    "        sri_data.infectious_data as data\n",
    "    JOIN\n",
    "        `sri_data.non_outliers_table` as nout\n",
    "    ON\n",
    "        data.run_id = nout.run_id\n",
    "    GROUP BY\n",
    "        date, \n",
    "        CENTROID_ID\n",
    "    ORDER BY\n",
    "        CENTROID_ID, \n",
    "        date;\n",
    "    \"\"\"\n",
    "plt_curves = client.query(get_curves).to_dataframe()  # Execute and fetch results\n",
    "print(\"Curves extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "get_mid_curves = \"\"\"-- Step 3: Calculate min and max values at each time step using the non-outliers table\n",
    "    SELECT\n",
    "        data.date,\n",
    "        mc.CENTROID_ID,\n",
    "        MAX(data.total_infectious) as curve_75,\n",
    "        MIN(data.total_infectious) as curve_25\n",
    "    FROM\n",
    "        sri_data.infectious_data as data\n",
    "    JOIN\n",
    "        `sri_data.middle_curves` as mc\n",
    "    ON\n",
    "        data.run_id = mc.run_id\n",
    "    GROUP BY\n",
    "        date, \n",
    "        CENTROID_ID\n",
    "    ORDER BY\n",
    "        CENTROID_ID, \n",
    "        date;\n",
    "    \"\"\"\n",
    "plt_middle = client.query(get_mid_curves).to_dataframe()  # Execute and fetch results\n",
    "print(\"Curves extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37939fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outliers = \"\"\"-- Step 3: Calculate min and max values at each time step using the non-outliers table\n",
    "    WITH outliers AS (\n",
    "        SELECT \n",
    "            tdt.run_id, \n",
    "            tdt.CENTROID_ID\n",
    "        FROM \n",
    "            `sri_data.total_distances_table` as tdt\n",
    "        WHERE \n",
    "            run_id NOT IN (SELECT run_id FROM `sri_data.non_outliers_table`)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    SELECT\n",
    "        data.date,\n",
    "        outliers.CENTROID_ID,\n",
    "        outliers.run_id,\n",
    "        data.total_infectious\n",
    "    FROM\n",
    "        outliers\n",
    "    JOIN\n",
    "        sri_data.infectious_data as data\n",
    "    ON\n",
    "        data.run_id = outliers.run_id\n",
    "    ORDER BY\n",
    "        run_id, \n",
    "        date;\n",
    "    \"\"\"\n",
    "plt_outliers = client.query(get_outliers).to_dataframe()  # Execute and fetch results\n",
    "print(\"Curves extracted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78642926",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "# here are all of our outliers\n",
    "px.line(plt_outliers, x='date', y='total_infectious', color='CENTROID_ID', line_group='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging plt_curves and plt_middle into one dataframe because I can\n",
    "merged_curves = pd.merge(plt_curves, plt_middle, on=['date', 'CENTROID_ID'], how='inner')\n",
    "merged_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23848ec9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create and lay out graph\n",
    "fig = go.Figure()\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f\"<b>Functional Boxplot</b><br><span style='font-size: 12px;'>As seen in \\\n",
    "<a href=https://www.tandfonline.com/doi/pdf/10.1198/jcgs.2011.09224?casa_token=ID3IjHflKz4AAAAA:4i-zhPbXhDzg\\\n",
    "8pDuowEPWoNiUFzHFcADAHHsqPonc6ac4dIzuQ40g5VA_n4BlUU7v1JsW7OD7Hf2>Sun & Genton (2011)</a>.  \\\n",
    "Clusters: {num_clusters}, features: {num_features}, grouping: {grouping_method}, centrality: {centrality_method}.</span>\",\n",
    "        'x': 0.5,  \n",
    "        'y': 0.9,  \n",
    "    },\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Incidence\",\n",
    "    \n",
    "    # colors\n",
    "    xaxis=dict(gridcolor='#EFEFEF'),  # Change x-axis grid color\n",
    "    yaxis=dict(gridcolor='#EFEFEF'),   # Change y-axis grid color\n",
    "    plot_bgcolor='#FFFFFF',\n",
    "    paper_bgcolor='#FFFFFF',\n",
    ")\n",
    "# fig.update_xaxes(range=[pd.Timestamp(\"2009-09-01\"), pd.Timestamp(\"2010-02-17\")])\n",
    "# fig.update_yaxes(range=[0, 35000])\n",
    "colors = 'maroon', 'navy', 'green', 'purple'\n",
    "\n",
    "# plot outliers\n",
    "for run in plt_outliers['run_id'].unique():\n",
    "        df_run = plt_outliers[plt_outliers['run_id'] == run]\n",
    "        gr = df_run.iloc[0, 1] # careful not to change table formatting\n",
    "    \n",
    "        fig.add_trace(go.Scatter(\n",
    "        name=f'Group {gr} Outlier',\n",
    "        x=df_run['date'],\n",
    "        y=df_run['total_infectious'],\n",
    "        marker=dict(color=color_to_rgba(colors[gr-1], .5, alpha=.3)),\n",
    "        line=dict(width=1, dash='solid'),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(gr)  # Assign to legend group\n",
    "    ))\n",
    "\n",
    "for group in plt_median['CENTROID_ID'].unique():\n",
    "    print(group)\n",
    "    # actually graph\n",
    "    # Lower\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Lower Bound',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_0'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    # Upper\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Upper Bound',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_100'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba(colors[group-1], 1, .3),\n",
    "        fill='tonexty',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "        \n",
    "    # Lower\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Lower Quartile',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_25'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    # Upper\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group}',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_75'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba(colors[group-1], 1, alpha=.3),\n",
    "        fill='tonexty',\n",
    "        showlegend=True,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Median',\n",
    "        x=plt_median[plt_median['CENTROID_ID'] == group]['date'],\n",
    "        y=plt_median[plt_median['CENTROID_ID'] == group]['median'],\n",
    "        marker=dict(color=color_to_rgba(colors[group-1], 1, alpha=1)),\n",
    "        line=dict(width=1),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fa5bb",
   "metadata": {},
   "source": [
    "## Fixed time quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668987d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_time_quantiles = \"\"\"\n",
    "WITH daily_data AS (\n",
    "    SELECT \n",
    "        date, \n",
    "        total_infectious,\n",
    "        run_id,\n",
    "        ROW_NUMBER() OVER (PARTITION BY date ORDER BY total_infectious) AS row_num,\n",
    "        COUNT(*) OVER (PARTITION BY date) AS total_rows\n",
    "    FROM sri_data.infectious_data\n",
    "),\n",
    "\n",
    "-- Joining with kmeans_results to attach CENTROID_ID\n",
    "centroid_data AS (\n",
    "    SELECT \n",
    "        d.date,\n",
    "        d.total_infectious,\n",
    "        d.run_id,\n",
    "        k.CENTROID_ID\n",
    "    FROM daily_data d\n",
    "    JOIN `sri_data.kmeans_results` k ON d.run_id = k.run_id -- Adjust the join condition if necessary\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    CENTROID_ID,\n",
    "    date,\n",
    "    PERCENTILE_CONT(total_infectious, 0) OVER (PARTITION BY CENTROID_ID, date) AS Min,\n",
    "    PERCENTILE_CONT(total_infectious, 0.05) OVER (PARTITION BY CENTROID_ID, date) AS Perc5,\n",
    "    PERCENTILE_CONT(total_infectious, 0.25) OVER (PARTITION BY CENTROID_ID, date) AS Q1,\n",
    "    PERCENTILE_CONT(total_infectious, 0.50) OVER (PARTITION BY CENTROID_ID, date) AS Median,\n",
    "    PERCENTILE_CONT(total_infectious, 0.75) OVER (PARTITION BY CENTROID_ID, date) AS Q3,\n",
    "    PERCENTILE_CONT(total_infectious, 0.95) OVER (PARTITION BY CENTROID_ID, date) AS Perc95,\n",
    "    PERCENTILE_CONT(total_infectious, 1) OVER (PARTITION BY CENTROID_ID, date) AS Max\n",
    "FROM centroid_data\n",
    "GROUP BY CENTROID_ID, date, total_infectious\n",
    "ORDER BY CENTROID_ID, date;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "plt_ftq = client.query(fixed_time_quantiles).result().to_dataframe()  # Execute the query to create the table\n",
    "print(\"Data pulled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b238f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt_ftq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c563bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a monstrosity of a query\n",
    "get_outlying_points = \"\"\"\n",
    "WITH daily_data AS (\n",
    "    SELECT \n",
    "        date, \n",
    "        total_infectious,\n",
    "        run_id\n",
    "    FROM sri_data.infectious_data  \n",
    "),\n",
    "\n",
    "centroid_data AS (\n",
    "    SELECT \n",
    "        d.date,\n",
    "        d.total_infectious,\n",
    "        d.run_id,\n",
    "        k.CENTROID_ID\n",
    "    FROM daily_data d\n",
    "    JOIN `sri_data.kmeans_results` k ON d.run_id = k.run_id\n",
    "),\n",
    "\n",
    "percentile_data AS (\n",
    "    SELECT \n",
    "        CENTROID_ID,\n",
    "        date,\n",
    "        PERCENTILE_CONT(total_infectious, 0.05) OVER (PARTITION BY CENTROID_ID, date) AS Perc5,\n",
    "        PERCENTILE_CONT(total_infectious, 0.95) OVER (PARTITION BY CENTROID_ID, date) AS Perc95\n",
    "    FROM centroid_data\n",
    "    GROUP BY CENTROID_ID, date, total_infectious\n",
    ")\n",
    "\n",
    "-- Main query to filter points outside the 90% interval\n",
    "SELECT DISTINCT\n",
    "    cd.CENTROID_ID,\n",
    "    cd.date,\n",
    "    cd.total_infectious\n",
    "FROM centroid_data cd\n",
    "JOIN percentile_data pd\n",
    "  ON cd.CENTROID_ID = pd.CENTROID_ID\n",
    "  AND cd.date = pd.date\n",
    "WHERE cd.total_infectious < pd.Perc5  -- Below 5th percentile\n",
    "   OR cd.total_infectious > pd.Perc95  -- Above 95th percentile\n",
    "ORDER BY cd.CENTROID_ID, cd.date;\n",
    "\n",
    "\"\"\"\n",
    "plt_outlying_points = client.query(get_outlying_points).result().to_dataframe()  # Execute the query to create the table\n",
    "print(\"Data pulled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb20e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(plt_outlying_points, x='date', y='total_infectious', color=plt_outlying_points['CENTROID_ID'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fbb38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_range = False\n",
    "outlying_points = True\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f\"<b>Traditional Boxplot</b><br><span style='font-size: 12px;'>Uses fixed-time quantiles.</span>\",\n",
    "        'x': 0.5,  \n",
    "        'y': 0.9,  \n",
    "    },\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Incidence\",\n",
    "    \n",
    "    # colors\n",
    "    xaxis=dict(gridcolor='#EFEFEF'),  # Change x-axis grid color\n",
    "    yaxis=dict(gridcolor='#EFEFEF'),   # Change y-axis grid color\n",
    "    plot_bgcolor='#FFFFFF',\n",
    "    paper_bgcolor='#FFFFFF',\n",
    ")\n",
    "# fig.update_xaxes(range=[pd.Timestamp(\"2009-09-01\"), pd.Timestamp(\"2010-02-17\")])\n",
    "# fig.update_yaxes(range=[0, 35000])\n",
    "\n",
    "try:\n",
    "    plt_ftq.set_index('date', inplace=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "for group in plt_ftq['CENTROID_ID'].unique():\n",
    "    df_group = plt_ftq[plt_ftq['CENTROID_ID'] == group]\n",
    "\n",
    "    if full_range:\n",
    "        # FULL RANGE\n",
    "        fig.add_trace(go.Scatter(\n",
    "            name=f'Minimum',\n",
    "            x=df_group.index,\n",
    "            y=df_group['Min'],\n",
    "            marker=dict(color=\"#444\"),\n",
    "            line=dict(width=0),\n",
    "            mode='lines',\n",
    "            showlegend=False,\n",
    "            legendgroup=str(group)  # Assign to legend group\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            name=f'Full Range',\n",
    "            x=df_group.index,\n",
    "            y=df_group['Max'],\n",
    "            mode='lines',\n",
    "            marker=dict(color=\"#444\"),\n",
    "            line=dict(width=0),\n",
    "            fillcolor=color_to_rgba(colors[group-1], .4, .2),\n",
    "            fill='tonexty',\n",
    "            showlegend=False,\n",
    "            legendgroup=str(group)  # Assign to legend group\n",
    "        ))\n",
    "    \n",
    "    \n",
    "    # MIDDLE 90%\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Minimum',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Perc5'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Middle 90%',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Perc95'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba(colors[group-1], .6, .2),\n",
    "        fill='tonexty',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "    # MIDDLE 50%\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Minimum',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Q1'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group}',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Q3'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba(colors[group-1], .8, .3),\n",
    "        fill='tonexty',\n",
    "        showlegend=True,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Median',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Median'],\n",
    "        marker=dict(color=color_to_rgba(colors[group-1], 1, 1)),\n",
    "        line=dict(width=1),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)\n",
    "    ))\n",
    "    \n",
    "    if outlying_points:\n",
    "        fig.add_trace(go.Scatter(\n",
    "        name=f'Outlying Points',\n",
    "        x=plt_outlying_points[plt_outlying_points['CENTROID_ID'] == group]['date'],\n",
    "        y=plt_outlying_points[plt_outlying_points['CENTROID_ID'] == group]['total_infectious'],\n",
    "        mode='markers',\n",
    "        marker=dict(color=color_to_rgba(colors[group-1], .4, .1)),\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212b08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420cc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a new figure to overlay both graphs\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces from the functional boxplot\n",
    "for run in plt_outliers['run_id'].unique():\n",
    "    df_run = plt_outliers[plt_outliers['run_id'] == run]\n",
    "    gr = df_run.iloc[0, 1]  # careful not to change table formatting\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {gr} Outlier (Functional)',\n",
    "        x=df_run['date'],\n",
    "        y=df_run['total_infectious'],\n",
    "        marker=dict(color=color_to_rgba('maroon', .5, alpha=.3)),\n",
    "        line=dict(width=1, dash='solid'),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup='Functional'  # Assign to functional legend group\n",
    "    ))\n",
    "\n",
    "for group in plt_median['CENTROID_ID'].unique():\n",
    "    # Lower Bound\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Lower Bound (Functional)',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_0'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup='Functional'  # Assign to functional legend group\n",
    "    ))\n",
    "    # Upper Bound\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Functional Boxplot',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_100'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba('maroon', 1, .3),\n",
    "        fill='tonexty',\n",
    "        showlegend=True,\n",
    "        legendgroup='Functional'  # Assign to functional legend group\n",
    "    ))\n",
    "\n",
    "    # Other traces (lower quartile, upper quartile, median)\n",
    "    # Lower Quartile\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Lower Quartile (Functional)',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_25'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup='Functional'  # Assign to functional legend group\n",
    "    ))\n",
    "    # Upper Quartile\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Upper Quartile (Functional)',\n",
    "        x=merged_curves[merged_curves['CENTROID_ID'] == group]['date'],\n",
    "        y=merged_curves[merged_curves['CENTROID_ID'] == group]['curve_75'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba('maroon', 1, alpha=.3),\n",
    "        fill='tonexty',\n",
    "        showlegend=False,\n",
    "        legendgroup='Functional'  # Assign to functional legend group\n",
    "    ))\n",
    "\n",
    "    # Median\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Median (Functional)',\n",
    "        x=plt_median[plt_median['CENTROID_ID'] == group]['date'],\n",
    "        y=plt_median[plt_median['CENTROID_ID'] == group]['median'],\n",
    "        marker=dict(color=color_to_rgba('maroon', 1, alpha=1)),\n",
    "        line=dict(width=1),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup='Functional'  # Assign to functional legend group\n",
    "    ))\n",
    "\n",
    "# Add traces from the traditional boxplot\n",
    "for group in plt_ftq['CENTROID_ID'].unique():\n",
    "    df_group = plt_ftq[plt_ftq['CENTROID_ID'] == group]\n",
    "\n",
    "    # Full Range\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         name=f'Traditional',\n",
    "#         x=df_group.index,\n",
    "#         y=df_group['Min'],\n",
    "#         marker=dict(color=\"#444\"),\n",
    "#         line=dict(width=0),\n",
    "#         mode='lines',\n",
    "#         showlegend=False,\n",
    "#         legendgroup='Traditional'  # Assign to traditional legend group\n",
    "#     ))\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         name=f'Traditional Boxplot',\n",
    "#         x=df_group.index,\n",
    "#         y=df_group['Max'],\n",
    "#         mode='lines',\n",
    "#         marker=dict(color=\"#444\"),\n",
    "#         line=dict(width=0),\n",
    "#         fillcolor=color_to_rgba('navy', .4, .2),\n",
    "#         fill='tonexty',\n",
    "#         showlegend=True,\n",
    "#         legendgroup='Traditional'  # Assign to traditional legend group\n",
    "#     ))\n",
    "\n",
    "    # Middle 90%\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Traditional Boxplot',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Perc5'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup='Traditional'  # Assign to traditional legend group\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Traditional Boxplot',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Perc95'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba('navy', .6, .2),\n",
    "        fill='tonexty',\n",
    "        showlegend=True,\n",
    "        legendgroup='Traditional'  # Assign to traditional legend group\n",
    "    ))\n",
    "\n",
    "    # Middle 50%\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Minimum (Traditional)',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Q1'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup='Traditional'  # Assign to traditional legend group\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Middle 50% (Traditional)',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Q3'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba('navy', .8, .3),\n",
    "        fill='tonexty',\n",
    "        showlegend=False,\n",
    "        legendgroup='Traditional'  # Assign to traditional legend group\n",
    "    ))\n",
    "\n",
    "    # Median\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Median (Traditional)',\n",
    "        x=df_group.index,\n",
    "        y=df_group['Median'],\n",
    "        marker=dict(color=color_to_rgba('navy', 1, 1)),\n",
    "        line=dict(width=1),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup='Traditional'  # Assign to traditional legend group\n",
    "    ))\n",
    "\n",
    "    # Outlying Points\n",
    "    if outlying_points:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            name=f'Outlying Points (Traditional)',\n",
    "            x=plt_outlying_points[plt_outlying_points['CENTROID_ID'] == group]['date'],\n",
    "            y=plt_outlying_points[plt_outlying_points['CENTROID_ID'] == group]['total_infectious'],\n",
    "            mode='markers',\n",
    "            marker=dict(color=color_to_rgba('navy', .4, .1)),\n",
    "            showlegend=False,\n",
    "            legendgroup='Traditional'  # Assign to traditional legend group\n",
    "        ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"<b>Combined Boxplots</b><br><span style='font-size: 12px;'>Functional and Traditional Boxplots.</span>\",\n",
    "        'x': 0.5,\n",
    "        'y': 0.9,\n",
    "    },\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Incidence\",\n",
    "    xaxis=dict(gridcolor='#EFEFEF'),\n",
    "    yaxis=dict(gridcolor='#EFEFEF'),\n",
    "    plot_bgcolor='#FFFFFF',\n",
    "    paper_bgcolor='#FFFFFF',\n",
    ")\n",
    "\n",
    "# Show the combined figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74155f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c9021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2fedb0",
   "metadata": {},
   "source": [
    "## Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # Set the number of times to test each query\n",
    "# k = 10\n",
    "# num_clusters = 1\n",
    "# num_features = 5\n",
    "# centrality_method = 'mbd'\n",
    "# grouping_method = 'mse'\n",
    "\n",
    "# # List of queries to test with meaningful names\n",
    "# queries = {\n",
    "#     \"Create Infectious Data Table\": query_base,\n",
    "#     \"Create Curve Distances Table\": query_distances,\n",
    "#     \"Create Distance Matrix Table\": query_distance_matrix,\n",
    "#     \"Assign All Runs to One Cluster\" if num_clusters == 1 else \"Create KMeans Model\": query_assign_all_to_one_cluster if num_clusters == 1 else query_create_model,\n",
    "#     \"KMeans Clustering\" if num_clusters > 1 else None: query_kmeans if num_clusters > 1 else None,  # Only add KMeans clustering if num_clusters > 1\n",
    "#     \"Save Sum Distances (ABC or MSE)\" if centrality_method in ['abc', 'mse'] else \"Calculate MBD\": save_sum_distances if centrality_method in ['abc', 'mse'] else mbd,\n",
    "#     \"Identify Non-Outliers\": query_non_outliers,\n",
    "#     \"Save Middle Curves\": query_middle_curves,\n",
    "#     \"Save Median Curves\": save_median,\n",
    "#     \"Get Median Curves\": get_median_curves,\n",
    "#     \"Get Overall Curves\": get_curves,\n",
    "#     \"Get Middle Curves\": get_mid_curves,\n",
    "#     \"Get Outliers\": get_outliers\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a484c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Filter out any None queries (from conditional logic above)\n",
    "# queries = {name: q for name, q in queries.items() if q is not None}\n",
    "\n",
    "# # Dictionary to store the execution times\n",
    "# execution_times = {name: [] for name in queries.keys()}\n",
    "\n",
    "# # Test each query k times and record execution times\n",
    "# for query_name, query in queries.items():\n",
    "# #     print(f\"\\nTesting '{query_name}'...\")\n",
    "\n",
    "#     # Create a progress bar for the runs of the current query\n",
    "#     for j in tqdm(range(k), desc=f\"{query_name} runs\", leave=False):\n",
    "#         start_time = time.time()\n",
    "#         client.query(query).result()  # Execute the query\n",
    "#         elapsed_time = time.time() - start_time\n",
    "        \n",
    "#         # Store the elapsed time\n",
    "#         execution_times[query_name].append(elapsed_time)\n",
    "# #         print(f\"Run {j + 1}/{k} for '{query_name}' took {round(elapsed_time, 3)} seconds.\")\n",
    "\n",
    "# #     print(f\"Completed '{query_name}'\")\n",
    "\n",
    "# # Convert execution times to a DataFrame for easier analysis\n",
    "# df_times = pd.DataFrame.from_dict(execution_times, orient='index').T\n",
    "\n",
    "# # Summary statistics\n",
    "# summary = df_times.describe().T[['mean', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import plotly.graph_objects as go\n",
    "# from scipy.stats import gaussian_kde\n",
    "# # Melt the DataFrame to long format for Plotly\n",
    "# df_melted = df_times.reset_index().melt(id_vars='index', var_name='Query', value_name='Execution Time')\n",
    "# df_melted.rename(columns={'index': 'Run'}, inplace=True)\n",
    "\n",
    "# # Calculate total execution time per run\n",
    "# total_time_per_run = df_melted.groupby('Run')['Execution Time'].sum().reset_index()\n",
    "\n",
    "# # Create the figure\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # Add a KDE plot for each query\n",
    "# for query in df_melted['Query'].unique():\n",
    "#     query_data = df_melted[df_melted['Query'] == query]['Execution Time']\n",
    "    \n",
    "#     # Compute the KDE for each query\n",
    "#     kde = gaussian_kde(query_data, bw_method='scott')  # Adjust the bandwidth method\n",
    "#     x_values = np.linspace(min(query_data), max(query_data), 100)\n",
    "#     kde_values = kde(x_values)\n",
    "\n",
    "#     # Add the KDE trace with grouped legend\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=x_values,\n",
    "#         y=kde_values,\n",
    "#         mode='lines',\n",
    "#         name=query,\n",
    "#         legendgroup=query,  # Use the category map\n",
    "#         line=dict(width=2),  # Adjust line width\n",
    "#     ))\n",
    "\n",
    "# # Add a trace for the total execution time per run\n",
    "# kde_total = gaussian_kde(total_time_per_run['Execution Time'], bw_method='scott')  # KDE on total times\n",
    "# total_x_values = np.linspace(min(total_time_per_run['Execution Time']), max(total_time_per_run['Execution Time']), 100)\n",
    "# total_kde_values = kde_total(total_x_values)\n",
    "\n",
    "# # # Add the total time KDE trace\n",
    "# # fig.add_trace(go.Scatter(\n",
    "# #     x=total_x_values,\n",
    "# #     y=total_kde_values,\n",
    "# #     mode='lines',\n",
    "# #     name='Total Execution Time',\n",
    "# #     legendgroup='Total Time',  # Grouped under a specific legend group\n",
    "# #     line=dict(width=3, color='red'),  # Adjust color and width for distinction\n",
    "# # ))\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title='Smoothed Probability Estimates of Query Execution Times',\n",
    "#     xaxis_title='Execution Time (seconds)',\n",
    "#     yaxis_title='Density',\n",
    "#     legend_title='Query Types',\n",
    "#     template='plotly_white',\n",
    "# )\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
