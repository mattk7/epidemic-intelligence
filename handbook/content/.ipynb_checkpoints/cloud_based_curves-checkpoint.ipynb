{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82146fbf",
   "metadata": {},
   "source": [
    "# Everything is on the cloud now and I feel numb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3677064",
   "metadata": {},
   "source": [
    "There's something off about the distance calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb619db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f143e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the location of our credentials json and name of bigquery project\n",
    "credentials = service_account.Credentials.from_service_account_file('C:\\\\Users\\\\elija\\\\Documents\\\\24f-coop\\\\credentials.json')\n",
    "project = 'net-data-viz-handbook'\n",
    "\n",
    "# Initialize a GCS client\n",
    "client = bigquery.Client(credentials=credentials, project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c40e78",
   "metadata": {},
   "source": [
    "## Getting df_pivot for graphing porpoises\n",
    "This will happen 1) much later in the process and 2) on Google Cloud, but I just want to have easy access to it for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395d2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pivot just because it's nice to have\n",
    "query_pivot = \"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT \n",
    "        date,\n",
    "        country_id,\n",
    "        run_id,\n",
    "        SUM(Infectious_13_17 + Infectious_18_23) AS total_infectious\n",
    "    FROM `net-data-viz-handbook.sri_data.SIR_0_countries_incidence_daily`\n",
    "    WHERE country_id IN (215)\n",
    "      AND run_id BETWEEN 1 AND 100\n",
    "    GROUP BY date, country_id, run_id\n",
    ")\n",
    "PIVOT (\n",
    "    SUM(total_infectious) FOR run_id IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n",
    "                                         21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, \n",
    "                                         41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, \n",
    "                                         61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, \n",
    "                                         81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)\n",
    ")\n",
    "ORDER BY date;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89f7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elija\\anaconda3\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-02-17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1    2    3    4    5    6    7    8    9    10   ...  91   92   \\\n",
       "date                                                          ...             \n",
       "2009-02-17    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "2009-02-18    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "2009-02-19    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "2009-02-20    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "2009-02-21    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "\n",
       "            93   94   95   96   97   98   99   100  \n",
       "date                                                \n",
       "2009-02-17    0    0    0    0    0    0    0    0  \n",
       "2009-02-18    0    0    0    0    0    0    0    0  \n",
       "2009-02-19    0    0    0    0    0    0    0    0  \n",
       "2009-02-20    0    0    0    0    0    0    0    0  \n",
       "2009-02-21    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the query\n",
    "query_job = client.query(query_pivot)\n",
    "\n",
    "# Fetch the results into a DataFrame\n",
    "df_pivot = query_job.to_dataframe()\n",
    "\n",
    "del df_pivot['country_id']\n",
    "df_pivot.set_index('date', inplace=True)\n",
    "\n",
    "# this is gonna be hell in SQL\n",
    "df_pivot.columns = df_pivot.columns.str.replace('_', '', regex=False).astype(int)\n",
    "\n",
    "# Display the first few rows\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bb745",
   "metadata": {},
   "source": [
    "## KMeans and a prayer\n",
    "- Uses the first num_features columns to simplify the KMeans algorithms, which is O(n<sup>2</sup>)\n",
    "- Procedural once distance matrix is computed (eg have to query main table manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69709952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE matrix table created successfully.\n",
      "ABC matrix table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the MSE matrix table\n",
    "query_mse_matrix = \"\"\"\n",
    "CREATE OR REPLACE TABLE sri_data.mse_matrix AS\n",
    "WITH infectious_data AS (\n",
    "    SELECT\n",
    "        date,\n",
    "        country_id,\n",
    "        run_id,\n",
    "        SUM(Infectious_13_17 + Infectious_18_23) AS total_infectious\n",
    "    FROM\n",
    "        net-data-viz-handbook.sri_data.SIR_0_countries_incidence_daily\n",
    "    WHERE\n",
    "        country_id IN (215) AND run_id BETWEEN 1 AND 100\n",
    "    GROUP BY\n",
    "        date,\n",
    "        country_id,\n",
    "        run_id\n",
    ")\n",
    "SELECT\n",
    "    a.run_id AS run_id_a,\n",
    "    b.run_id AS run_id_b,\n",
    "    AVG(POW(a.total_infectious - b.total_infectious, 2)) AS mse\n",
    "FROM\n",
    "    infectious_data a\n",
    "JOIN\n",
    "    infectious_data b\n",
    "ON\n",
    "    a.date = b.date\n",
    "GROUP BY\n",
    "    run_id_a, run_id_b\n",
    "\"\"\"\n",
    "client.query(query_mse_matrix).result()  # Execute the query to create the table\n",
    "print(\"MSE matrix table created successfully.\")\n",
    "\n",
    "# Step 2: Create the ABC matrix table\n",
    "query_abc_matrix = \"\"\"\n",
    "CREATE OR REPLACE TABLE sri_data.abc_matrix AS\n",
    "WITH infectious_data AS (\n",
    "    SELECT\n",
    "        date,\n",
    "        country_id,\n",
    "        run_id,\n",
    "        SUM(Infectious_13_17 + Infectious_18_23) AS total_infectious\n",
    "    FROM\n",
    "        net-data-viz-handbook.sri_data.SIR_0_countries_incidence_daily\n",
    "    WHERE\n",
    "        country_id IN (215) AND run_id BETWEEN 1 AND 100\n",
    "    GROUP BY\n",
    "        date,\n",
    "        country_id,\n",
    "        run_id\n",
    ")\n",
    "SELECT\n",
    "    a.run_id AS run_id_a,\n",
    "    b.run_id AS run_id_b,\n",
    "    SUM(ABS(a.total_infectious - b.total_infectious)) AS abc\n",
    "FROM\n",
    "    infectious_data a\n",
    "JOIN\n",
    "    infectious_data b\n",
    "ON\n",
    "    a.date = b.date\n",
    "GROUP BY\n",
    "    run_id_a, run_id_b\n",
    "\"\"\"\n",
    "client.query(query_abc_matrix).result()  # Execute the query to create the table\n",
    "print(\"ABC matrix table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3877b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create the distance matrix\n",
    "method = 'mse' # 'mse' or 'abc'\n",
    "\n",
    "query_distance_matrix = f\"\"\"\n",
    "CREATE OR REPLACE TABLE sri_data.distance_matrix AS\n",
    "SELECT\n",
    "    run_id_a AS run_id,\n",
    "        ARRAY_AGG(STRUCT(run_id_b, {method}) ORDER BY run_id_b ASC) AS distances\n",
    "FROM\n",
    "    sri_data.{method}_matrix\n",
    "GROUP BY\n",
    "    run_id_a;\n",
    "\"\"\"\n",
    "client.query(query_distance_matrix).result()  # Execute the query to create the table\n",
    "print(\"Distance matrix table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the K-means model\n",
    "num_clusters = 2  # Example number of clusters\n",
    "num_features = 5   # Set the number of features to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce686e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the K-means model by selecting the first num_features features based on actual distances\n",
    "query_create_model = f\"\"\"\n",
    "CREATE OR REPLACE MODEL sri_data.kmeans_model\n",
    "OPTIONS(model_type='kmeans', num_clusters={num_clusters}) AS\n",
    "SELECT\n",
    "    run_id,\n",
    "    ARRAY(\n",
    "        SELECT distance.{method} \n",
    "        FROM UNNEST(distances) AS distance \n",
    "        WHERE distance.run_id_b <= {num_features}  -- Select only the first num_features\n",
    "    ) AS features\n",
    "FROM\n",
    "    sri_data.distance_matrix;\n",
    "\"\"\"\n",
    "s = time.time()\n",
    "client.query(query_create_model).result()  # Execute the model creation\n",
    "print(f\"K-means model created successfully in {round(time.time() - s, 3)} seconds.\")\n",
    "\n",
    "# Step 5: Apply K-means clustering and save results in a table\n",
    "query_kmeans = f\"\"\"\n",
    "CREATE OR REPLACE TABLE sri_data.kmeans_results AS\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    ML.PREDICT(MODEL sri_data.kmeans_model,\n",
    "        (SELECT\n",
    "            run_id,\n",
    "            ARRAY(\n",
    "                SELECT distance.{method} \n",
    "                FROM UNNEST(distances) AS distance \n",
    "                WHERE distance.run_id_b <= {num_features}  \n",
    "            ) AS features\n",
    "         FROM\n",
    "            sri_data.distance_matrix)\n",
    "    ) AS predictions\n",
    "\"\"\"\n",
    "s = time.time()\n",
    "client.query(query_kmeans).result()  # Execute the model creation\n",
    "print(f\"K-means clustering results saved successfully in {round(time.time() - s, 3)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Sum distances within each cluster, second variation\n",
    "# \n",
    "percentile = 1 # not implemented currently, will let you select only most central subset of data\n",
    "\n",
    "query_sum_distances = f\"\"\"\n",
    "WITH a AS (\n",
    "    SELECT \n",
    "        kr.CENTROID_ID,\n",
    "        kr.run_id, \n",
    "        run_id_b, \n",
    "        {method}\n",
    "    FROM \n",
    "        `sri_data.kmeans_results` AS kr\n",
    "    JOIN \n",
    "        `sri_data.distance_matrix` AS dm\n",
    "    ON \n",
    "        kr.run_id = dm.run_id\n",
    "    CROSS JOIN \n",
    "        UNNEST(dm.distances) AS dm_dist  -- Unnest the distances array here \n",
    "),\n",
    "b AS (\n",
    "    SELECT\n",
    "        run_id AS run_id_b, \n",
    "        CENTROID_ID AS CENTROID_ID_B\n",
    "    FROM\n",
    "        `sri_data.kmeans_results`\n",
    ")\n",
    "SELECT \n",
    "    a.run_id,\n",
    "    a.CENTROID_ID,\n",
    "    AVG({method}) as total_distance\n",
    "FROM \n",
    "    a\n",
    "JOIN \n",
    "    b\n",
    "ON \n",
    "    a.run_id_b = b.run_id_b\n",
    "WHERE\n",
    "    a.CENTROID_ID = b.CENTROID_ID_B\n",
    "GROUP BY\n",
    "    a.CENTROID_ID,\n",
    "    a.run_id;\n",
    "\n",
    "\"\"\"\n",
    "total_distances = client.query(query_sum_distances).to_dataframe()  # Execute and fetch results\n",
    "print(\"Total distances summed within each cluster successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Define a list of colors for distinct centroids\n",
    "colors = [\n",
    "    'rgba(128, 0, 0, {alpha})',    # Red\n",
    "    'rgba(0, 0, 128, {alpha})',    # Blue\n",
    "    'rgba(0, 128, 0, {alpha})',    # Green\n",
    "    'rgba(128, 0, 128, {alpha})',  # Orange\n",
    "    'rgba(255, 0, 255, {alpha})',  # Magenta\n",
    "    'rgba(0, 255, 255, {alpha})',  # Cyan\n",
    "    'rgba(128, 0, 128, {alpha})',  # Purple\n",
    "    # Add more colors as needed\n",
    "]\n",
    "\n",
    "# Loop through each run_id to plot the curves\n",
    "for index, row in total_distances.iterrows():\n",
    "    run_id = row['run_id']\n",
    "    centroid_id = row['CENTROID_ID']\n",
    "    total_distance = row['total_distance']\n",
    "\n",
    "    # Get the curve data for the run_id from df_pivot\n",
    "    curve_data = df_pivot[run_id].rolling(7).median()\n",
    "\n",
    "    # Calculate alpha value based on total distance\n",
    "    alpha_value = max(0.1, 1 - (total_distance / total_distances['total_distance'].max()))  # Ensure alpha is between 0 and 1\n",
    "\n",
    "    # Select a color for the centroid\n",
    "    color = colors[centroid_id % len(colors) - 1].format(alpha=alpha_value)  # Loop through colors based on centroid_id\n",
    "\n",
    "    # Add the curve to the figure\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=curve_data.index,  # Assuming the x-axis is the index of df_pivot\n",
    "        y=curve_data.values,  # y-values from df_pivot\n",
    "        mode='lines',\n",
    "        name=f'Run ID {run_id}',\n",
    "        line=dict(color=color),  # Use the selected color\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Curves by Centroid ID with Distance-based Alpha',\n",
    "    xaxis_title='Date',  # Change this to your desired x-axis label\n",
    "    yaxis_title='Incidence',  # Change this to your desired y-axis label\n",
    "    legend_title='Run ID',\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17da5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_to_rgba(color_name, value, alpha):\n",
    "    # Get the RGB components of the color\n",
    "    rgb = mcolors.to_rgb(color_name)\n",
    "    \n",
    "    # Calculate the gray value based on the input value\n",
    "    gray = (1 - value) * 0.8  # This adjusts how gray the color will be\n",
    "\n",
    "    # Compute the final RGBA values\n",
    "    r = rgb[0] * value + gray\n",
    "    g = rgb[1] * value + gray\n",
    "    b = rgb[2] * value + gray\n",
    "    \n",
    "    # Return the RGBA string\n",
    "#     print(f'rgba({int(r * 255)}, {int(g * 255)}, {int(b * 255)}, {alpha})')\n",
    "    return f'rgba({int(r * 255)}, {int(g * 255)}, {int(b * 255)}, {alpha})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df13f4",
   "metadata": {},
   "source": [
    "## Making a Functional Boxplot\n",
    "This will be done group-wise (and in SQL, but I wanted to make sure it worked for today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61326a59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create and lay out graph\n",
    "fig = go.Figure()\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f\"<b>Functional Boxplot</b><br><span style='font-size: 12px;'>As seen in \\\n",
    "<a href=https://www.tandfonline.com/doi/pdf/10.1198/jcgs.2011.09224?casa_token=ID3IjHflKz4AAAAA:4i-zhPbXhDzg\\\n",
    "8pDuowEPWoNiUFzHFcADAHHsqPonc6ac4dIzuQ40g5VA_n4BlUU7v1JsW7OD7Hf2>Sun & Genton (2011)</a>.  \\\n",
    "Clusters: {num_clusters}, features: {num_features}, method: {method}.</span>\",\n",
    "        'x': 0.5,  \n",
    "        'y': 0.9,  \n",
    "    },\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Incidence\",\n",
    "\n",
    ")\n",
    "fig.update_xaxes(range=[pd.Timestamp(\"2009-09-01\"), pd.Timestamp(\"2010-02-17\")])\n",
    "\n",
    "# prepare graphing resources\n",
    "colors = 'maroon', 'navy', 'green', 'purple'\n",
    "df_graph = df_pivot.rolling(7).median()\n",
    "\n",
    "# do a bunch of data processing to get quartiles, will happen on SQL\n",
    "for group in total_distances['CENTROID_ID'].unique():\n",
    "    print(\"Group\", group)\n",
    "    df_group = total_distances[total_distances['CENTROID_ID'] == group]\n",
    "    print('Total Curves:', len(df_group))\n",
    "    \n",
    "    # Calculate quartiles\n",
    "    lower_quartile = df_group['total_distance'].quantile(.25)\n",
    "    upper_quartile = df_group['total_distance'].quantile(.75)\n",
    "    \n",
    "    # Calculate IQR\n",
    "    IQR = upper_quartile - lower_quartile\n",
    "    print('IQR:', round(IQR))\n",
    "    \n",
    "    # Filter rows within IQR (inner curves)\n",
    "    inner_curves = df_group.sort_values('total_distance').iloc[:int(len(df_group)/2), 0].to_list()\n",
    "    print('Inner Curves:', len(inner_curves))\n",
    "    print(inner_curves)\n",
    "    median = df_group.sort_values('total_distance').iloc[0, 0]\n",
    "    print('Median Curve:', median)\n",
    "    \n",
    "    curve_75 = df_graph[inner_curves].T.max()\n",
    "    curve_25 = df_graph[inner_curves].T.min()\n",
    "    \n",
    "    # Define outliers (beyond 1.5 * IQR)\n",
    "    lower_bound = lower_quartile - 1.5 * IQR\n",
    "    upper_bound = upper_quartile + 1.5 * IQR\n",
    "    print('Bounds:', round(lower_bound), round(upper_bound))\n",
    "    \n",
    "    # Filter to get outliers (those outside 1.5 IQRs)\n",
    "    outliers = df_group[(df_group['total_distance'] < lower_bound) | \\\n",
    "                        (df_group['total_distance'] > upper_bound)]['run_id'].to_list()\n",
    "    print('Outliers:', len(outliers))\n",
    "    print(outliers, '\\n')\n",
    "    \n",
    "    # Now calculate the curve_0 and curve_100, excluding outliers\n",
    "    non_outliers = df_group[~df_group['run_id'].isin(outliers)]['run_id'].to_list()\n",
    "    \n",
    "    curve_0 = df_graph[non_outliers].T.min()\n",
    "    curve_100 = df_graph[non_outliers].T.max()\n",
    "    \n",
    "    # actually graph\n",
    "    # Lower\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Lower Bound',\n",
    "        x=curve_0.index,\n",
    "        y=curve_0,\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    # Upper\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Upper Bound',\n",
    "        x=curve_100.index,\n",
    "        y=curve_100,\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba(colors[group-1], 1, .2),\n",
    "        fill='tonexty',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "        \n",
    "    # Lower\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Lower Quartile',\n",
    "        x=curve_25.index,\n",
    "        y=curve_25,\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    # Upper\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group}',\n",
    "        x=curve_75.index,\n",
    "        y=curve_75,\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=color_to_rgba(colors[group-1], 1, alpha=.2),\n",
    "        fill='tonexty',\n",
    "        showlegend=True,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "    for outlier in outliers:\n",
    "        fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Outlier',\n",
    "        x=df_graph.index,\n",
    "        y=df_graph[outlier],\n",
    "        marker=dict(color=color_to_rgba(colors[group-1], .5, alpha=.1)),\n",
    "        line=dict(width=1, dash='solid'),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group} Median',\n",
    "        x=df_graph.index,\n",
    "        y=df_graph[median],\n",
    "        marker=dict(color=color_to_rgba(colors[group-1], 1, alpha=1)),\n",
    "        line=dict(width=1),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04830a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_pivot.rolling(7).median(), alpha=.5, lw=.5, c='gray'); # remove this semicolon at your own risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a75f81",
   "metadata": {},
   "source": [
    "## KMeans runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Sample variables\n",
    "ls_num_clusters = [1, 2, 3, 10, 50]\n",
    "ls_num_features = [1, 3, 5, 10, 50, 100]\n",
    "i = 1  # Number of iterations\n",
    "\n",
    "# Dictionary to store runtimes\n",
    "runtime_data = {}\n",
    "\n",
    "# Loop over the number of iterations\n",
    "for num_clusters in ls_num_clusters:\n",
    "    for num_features in ls_num_features:\n",
    "        for _ in tqdm(range(i), desc=f'Clusters: {num_clusters} | Features: {num_features}', leave=False):\n",
    "            start_time = time.time()\n",
    "            # Simulate the query execution (replace this with your actual query)\n",
    "            client.query(query_create_model).result()  # Replace with actual query\n",
    "            end_time = time.time()\n",
    "            \n",
    "            runtime = end_time - start_time\n",
    "            \n",
    "            # Store runtime in the dictionary\n",
    "            key = (num_clusters, num_features)\n",
    "            if key in runtime_data:\n",
    "                runtime_data[key].append(runtime)\n",
    "            else:\n",
    "                runtime_data[key] = [runtime]\n",
    "\n",
    "# Calculate average runtimes\n",
    "average_runtimes = {\n",
    "    'num_clusters': [],\n",
    "    'num_features': [],\n",
    "    'average_runtime': []\n",
    "}\n",
    "\n",
    "for (num_clusters, num_features), runtimes in runtime_data.items():\n",
    "    average_runtimes['num_clusters'].append(num_clusters)\n",
    "    average_runtimes['num_features'].append(num_features)\n",
    "    average_runtimes['average_runtime'].append(sum(runtimes) / len(runtimes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
