{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import gzip\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07662a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_id = 'elijahsandler@net-data-viz-handbook.iam.gserviceaccount.com'\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\elija\\\\Documents\\\\24f-coop\\\\net-data-viz-handbook-fe2c5531555d.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d12e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a GCS client\n",
    "client = storage.Client()\n",
    "print('1 ')\n",
    "# Specify your bucket name and the specific .csv.gz file you want\n",
    "bucket_name = 'gs_net-data-viz-handbook'\n",
    "file_name = 'sample/sample_SIR_0_countries_incidence_daily.csv.gz'  # Update this to the specific file name\n",
    "meta_file = 'sample/sample_SIR_0_meta.csv.gz'\n",
    "print('2 ')\n",
    "# Get the bucket and blob\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(file_name)\n",
    "metablob = bucket.blob(meta_file)\n",
    "print('3 ')\n",
    "\n",
    "# Download the .csv.gz file as bytes\n",
    "compressed_content = blob.download_as_bytes()\n",
    "print('4 ')\n",
    "# Decompress the .csv.gz content\n",
    "with gzip.GzipFile(fileobj=io.BytesIO(compressed_content)) as gz:\n",
    "    # Read the decompressed content into a pandas DataFrame\n",
    "    df = pd.read_csv(gz)\n",
    "    \n",
    "# Download the .csv.gz file as bytes\n",
    "compressed_content = metablob.download_as_bytes()\n",
    "print('5 ')\n",
    "# Decompress the .csv.gz content\n",
    "with gzip.GzipFile(fileobj=io.BytesIO(compressed_content)) as gz:\n",
    "    # Read the decompressed content into a pandas DataFrame\n",
    "    df_meta = pd.read_csv(gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.iloc[4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb150656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df.drop(['t'], axis=1).groupby(['date', 'country_id', 'run_id']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2709f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only 1 country's data\n",
    "country =  0\n",
    "df_country = df_sum.loc[(slice(None), country), :]\n",
    "df_country = df_country.droplevel('country_id').T.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoting data. god what a good function.\n",
    "df_pivot = df_country.reset_index().pivot(index='date', columns='run_id', values=0).fillna(0)\n",
    "\n",
    "# zero-indexing run_id because we aren't barbarians\n",
    "df_pivot.columns = df_pivot.columns - 1 \n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_pivot,\n",
    "              labels={\n",
    "                     \"value\": \"Cases?\",\n",
    "                     \"date\": \"Date\",\n",
    "            },\n",
    "            title=\"2009/10 Incidence Spaghetti Plot\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ea9e2",
   "metadata": {},
   "source": [
    "## And just for fun...\n",
    "It's not really for fun because, you know, it's my job, but whatever. It's fun too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2895ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from curvestat import CurveBoxPlot\n",
    "from curvestat import LoadRisk\n",
    "from time import time\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from matplotlib.colors import to_rgba\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f14823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of time steps\n",
    "time_arr = np.arange(0,len(df_pivot),1)\n",
    "time_arr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de10545",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "Boxplot = CurveBoxPlot(curves=df_pivot,sample_curves=10,sample_repititions=40,time=time_arr)\n",
    "\n",
    "# Choose ranking\n",
    "rank_allornothing=Boxplot.rank_allornothing()\n",
    "rank_max = Boxplot.rank_max()\n",
    "\n",
    "# Get envelope with this choice of ranking\n",
    "boundaries = Boxplot.get_boundaries(rank_max,percentiles=[50,90])\n",
    "\n",
    "# ... HEATMAP : \n",
    "# First, define which curves we want the heatmap for\n",
    "heatmapcurves = list(boundaries['curve-based'][50]['curves'])\n",
    "\n",
    "# Then find the heatmap\n",
    "heatmap_50 = Boxplot.get_peakheatmap(heatmap_curves=heatmapcurves)\n",
    "\n",
    "# Plot results\n",
    "Boxplot.plot_everything()\n",
    "\n",
    "end = time()\n",
    "diff = end - start\n",
    "print(f\"Ran {len(df_pivot)} curves in {round(diff, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "# unfortunately, pairwise comparisons are O(n^2)\n",
    "curve_diff = dict()\n",
    "\n",
    "for first_curve in df_pivot.columns:\n",
    "    for second_curve in df_pivot.columns[first_curve+1:]:\n",
    "        curve_diff[(first_curve, second_curve)] = \\\n",
    "        ((df_pivot[first_curve] - df_pivot[second_curve])**1).abs().sum()\n",
    "end = time()\n",
    "print(f'pairwise compared {len(df_pivot)} curves in {round(end-start, 4)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique elements\n",
    "elements = sorted(set(i for pair in curve_diff.keys() for i in pair))\n",
    "n = len(elements)\n",
    "\n",
    "# Create the distance matrix\n",
    "distance_matrix = np.zeros((n, n))\n",
    "for (i, j), score in curve_diff.items():\n",
    "    distance_matrix[i, j] = score\n",
    "    distance_matrix[j, i] = score  # Because the matrix is symmetric\n",
    "\n",
    "# Convert distance matrix to similarity matrix\n",
    "similarity_matrix = np.max(distance_matrix) - distance_matrix\n",
    "\n",
    "# Fit K-Means clustering\n",
    "k = 3  # Change this to the number of desired clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18473e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'gray', 'cyan', 'yellow', 'gray']\n",
    "cmap = dict(zip(range(len(colors)), colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "# Plot each curve with its respective color based on the group\n",
    "for column, group in zip(df_pivot.columns, labels):\n",
    "    trace = go.Scatter(\n",
    "        x=df_pivot.index,\n",
    "        y=df_pivot[column],\n",
    "        mode='lines',\n",
    "        name=f'Curve {column}',\n",
    "        line=dict(color=cmap[group])\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "fig = go.Figure(traces)\n",
    "    \n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Grouped Incidence',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Cases? ',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = defaultdict(lambda: defaultdict())\n",
    "for label in np.unique(labels):\n",
    "    di[label]['data'] = df_pivot[[i for i in df_pivot.columns if labels[i] == label]]\n",
    "    \n",
    "    # create boxplot instance, add curves\n",
    "    Boxplot = CurveBoxPlot(curves=di[label]['data'],sample_curves=10,sample_repititions=500,time=time_arr)\n",
    "\n",
    "    # rank curves\n",
    "    rank_allornothing=Boxplot.rank_allornothing()\n",
    "    boundaries = Boxplot.get_boundaries(rank_allornothing,percentiles=[0, 50,90])\n",
    "    \n",
    "    di[label]['boundaries'] = boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_arr = df_pivot.index\n",
    "date_arr;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa956dce",
   "metadata": {},
   "source": [
    "This plot uses AUC for grouping curves, then `curvestat` for finding the boundaries for the median, middle quartiles, and middle 90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for group in di:\n",
    "    # Lower 5%\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Lower 5% of {group}',\n",
    "        x=date_arr,\n",
    "        y=di[group]['boundaries']['curve-based'][90]['min-boundary'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "    \n",
    "    # Upper 5%\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Upper 5% of {group}',\n",
    "        x=date_arr,\n",
    "        y=di[group]['boundaries']['curve-based'][90]['max-boundary'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor='rgba' + str(to_rgba(cmap[group], alpha=0.1)),\n",
    "        fill='tonexty',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "\n",
    "    # Lower Quartile\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Lower Quartile of {group}',\n",
    "        x=date_arr,\n",
    "        y=di[group]['boundaries']['curve-based'][50]['min-boundary'],\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "\n",
    "    # Upper Quartile\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Upper Quartile of {group}',\n",
    "        x=date_arr,\n",
    "        y=di[group]['boundaries']['curve-based'][50]['max-boundary'],\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor='rgba' + str(to_rgba(cmap[group], alpha=0.2)),\n",
    "        fill='tonexty',\n",
    "        showlegend=False,\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "\n",
    "    # Most Central Curve\n",
    "    fig.add_trace(go.Scatter(\n",
    "        name=f'Group {group}',\n",
    "        x=date_arr,\n",
    "        y=df_pivot[di[group]['boundaries']['curve-based'][0]['curves'][0]],\n",
    "        marker=dict(color=cmap[group]),\n",
    "        line=dict(width=1),\n",
    "        mode='lines',\n",
    "        showlegend=True,  # Show legend for the central curve\n",
    "        legendgroup=str(group)  # Assign to legend group\n",
    "    ))\n",
    "\n",
    "# Update layout to show the legend\n",
    "fig.update_layout(\n",
    "    title='Middle 90% Curves for Each Group',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Cases',\n",
    "    showlegend=True  # Enable legend\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd867d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(curve_diff, index=['Distance']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140273a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = {curve: 0 for curve in set(curve for pair in curve_diff.keys() for curve in pair)}\n",
    "\n",
    "# Compute centrality scores\n",
    "for (curve1, curve2), diff in curve_diff.items():\n",
    "    centrality[curve1] += diff\n",
    "    centrality[curve2] += diff\n",
    "\n",
    "# Convert centrality scores to a DataFrame for easy sorting\n",
    "centrality_df = pd.DataFrame(list(centrality.items()), columns=['Curve', 'Centrality'])\n",
    "centrality_df['Centrality'] = 1 - (centrality_df['Centrality']-centrality_df['Centrality'].min())/(centrality_df['Centrality'].max()-centrality_df['Centrality'].min())\n",
    "\n",
    "centrality_df['Group'] = labels\n",
    "centrality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank curves from most to least central, reminder that this is done by summing the area between all curves\n",
    "ranked_curves = centrality_df.sort_values(by='Centrality', ascending=False).reset_index(drop=True)\n",
    "ranked_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(x=ranked_curves['Centrality'], color=ranked_curves['Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68101aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pivot.T.quantile([0, .10, .25, .75, .90, 1]).T\n",
    "df['Median'] = df_pivot.T.median()\n",
    "df['Mean'] = df_pivot.T.mean()\n",
    "\n",
    "px.scatter(df, title='The Problem with Curve Box Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00579b",
   "metadata": {},
   "source": [
    "This graph illustrates the issue we face. There are times where the 75th percentile points look as though they are well above the 90th percentile points, which obviously isn't right. `curvestat` gets around this problem by basically just widening the confidence interval: now the 75% interval looks like the 90% interval. That doesn't help us though. At that point, we're deviating from the conventional use of statistics, and while I'm a big fan of breaking conventions, it is counter-productive with regards to our goal of making these graphs more legible. \n",
    "\n",
    "`curvestat`'s use of sampling also means that their package is not strictly deterministic, which is a big issue as well. This is rectified with my \"area under the curve\" method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one week rolling maximum for each run\n",
    "df_roll = df_pivot.rolling(7).max().dropna()\n",
    "\n",
    "# same as above\n",
    "df = df_roll.T.quantile([0, .10, .25, .75, .90, 1]).T\n",
    "df['Median'] = df_roll.T.median()\n",
    "df['Mean'] = df_roll.T.mean()\n",
    "px.scatter(df, title='The Problem with Curve Box Plots: Rollling Average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383990b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_pivot.max() < 1200).astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get rid of outliers then uhhhhh it looks great\n",
    "((df_pivot.rolling(7).median().dropna().max()) > 1200).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03b894",
   "metadata": {},
   "source": [
    "This seems to suggest that ~75% of the peaks are below 1200. Alas, 48% of them are. \n",
    "\n",
    "How is that possible? Well, the graph isn't suggesting that 90% of the peaks are below 1200. In any way. It's suggesting that at time Nov 19, 2009, 75% of curves will be below 1200, and at every other time, at least 75% of curves will be lower than that. Which is true! If you have a graph where the axis are date and value, then any given point on the graph represents the coincidence of the date and value: not the value and every date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687adf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9258e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df_pivot.max(), nbins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fd64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9e7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6052ff0",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Last shot: Let's group the curves using AUC, then calculate their centrality relative to <i>only the curves in their group</i> so we can cut `curvestat`'s BS out altogether and hopefully nail this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "# unfortunately, pairwise comparisons are O(n^2)\n",
    "curve_diff = dict()\n",
    "\n",
    "for first_curve in df_pivot.columns:\n",
    "    for second_curve in df_pivot.columns[first_curve+1:]:\n",
    "        curve_diff[(first_curve, second_curve)] = \\\n",
    "        ((df_pivot[first_curve] - df_pivot[second_curve])**1).abs().sum()\n",
    "\n",
    "\n",
    "# Get all unique elements\n",
    "elements = sorted(set(i for pair in curve_diff.keys() for i in pair))\n",
    "n = len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d1728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the distance matrix\n",
    "distance_matrix = np.zeros((n, n))\n",
    "for (i, j), score in curve_diff.items():\n",
    "    distance_matrix[i, j] = score\n",
    "    distance_matrix[j, i] = score  # Because the matrix is symmetric\n",
    "\n",
    "# Convert distance matrix to similarity matrix\n",
    "similarity_matrix = np.max(distance_matrix) - distance_matrix\n",
    "\n",
    "# Fit K-Means clustering\n",
    "k = 3  # Change this to the number of desired clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf3420",
   "metadata": {},
   "source": [
    "The curves are now grouped via AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "di[0]['data'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604938b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e092e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaeede8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba8824",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = {curve: 0 for curve in set(curve for pair in curve_diff.keys() for curve in pair)}\n",
    "\n",
    "# Compute centrality scores\n",
    "for (curve1, curve2), diff in curve_diff.items():\n",
    "    if labels[curve1] == labels[curve2]:\n",
    "        centrality[curve1] += diff\n",
    "        centrality[curve2] += diff\n",
    "\n",
    "# Convert centrality scores to a DataFrame for easy sorting\n",
    "centrality_df = pd.DataFrame(list(centrality.items()), columns=['Curve', 'Distance'])\n",
    "# centrality_df['Centrality'] = 1 - (centrality_df['Centrality']-centrality_df['Centrality'].min())/(centrality_df['Centrality'].max()-centrality_df['Centrality'].min())\n",
    "\n",
    "centrality_df['Group'] = labels\n",
    "centrality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd25510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_central_curves(percentile, group, df=centrality_df):\n",
    "    \"\"\" gets `percentile` most central curves from given group\"\"\"\n",
    "    \n",
    "    df = df[df['Group'] == group]\n",
    "    df.sort_values('Distance', inplace=True)\n",
    "    \n",
    "    ls = list(df.reset_index().loc[:max(0, ((percentile/100) * len(df) - 1)), 'Curve'])\n",
    "    \n",
    "#     return df_pivot[ls]\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_central_curves(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time()\n",
    "print(f'Ran process on {len(df_pivot)} curves in {round(end-start, 4)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59c631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d157567",
   "metadata": {},
   "source": [
    "# Creating a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40129495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37329f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df_pivot, color=['blue']*len(df_pivot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad88ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dict(zip(date_arr, time_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.groupby('date').sum()[df_country.groupby('date').sum()[0] != 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_country = df_country[df_country['date'].isin(df_country.groupby('date').sum()[df_country.groupby('date').sum()[0] != 0].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_time = filtered_df_country['date'].map(dt).values\n",
    "peak_value = filtered_df_country[0].values\n",
    "\n",
    "\n",
    "# Calculate point density\n",
    "xy = np.vstack([peak_time,peak_value])\n",
    "peak_density = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density, so that the densest points are plotted last\n",
    "idx = peak_density.argsort()\n",
    "peak_heatmap = {'peak_time':peak_time[idx], 'peak_value':peak_value[idx],'peak_density': peak_density[idx]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08938b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(peak_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(peak_heatmap['peak_density']**.0000002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=peak_heatmap['peak_time'], y=peak_heatmap['peak_value'], c=peak_heatmap['peak_density']**.0000010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.DataFrame(peak_heatmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df32a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
